{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4637fed-e92b-464a-aa28-172e2a97ec2e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Initialization Info"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "YB_HOST='[YB HOSTNAME]'                         #Ex: ybhost.elb.us-east-2.amazonaws.com\n",
    "YB_USER=\"[YB USERNAME]\"                         #Ex: JoeDBA@yellowbrickcloud.com\n",
    "YB_PASSWORD=\"[YB PASSWORD]\"                     #Ex: SuperS3cret!\n",
    "YB_DATABASE=\"[YB DATABASE]\"                     #Ex: proddb\n",
    "YB_SCHEMA=\"[YB SCHEMA]\"                         #Ex: gold\n",
    "\n",
    "DB_DATABASE = \"[DATABRICKS CATALOG]\"            #Ex: corp\n",
    "DB_SCHEMA = \"[DATABRICKS SCHEMA]\"               #Ex: silver\n",
    "\n",
    "SPARK_PARITIONS = 20                            #Number of files generated, can increase if very large dataset\n",
    "MIN_S3_ROWS = 1000000                           #Minimum number of rows for us to use S3, otherwise, use spark write\n",
    "\n",
    "#AWS Information if using S3 for Staging.  Set useS3 to False if you want to stream directly\n",
    "BUCKET_NAME = \"[BUCKET NAME (NO URL INFO)]\"     #Ex: mybucket\n",
    "AWS_ACCESS_KEY_ID = \"[ACCESS KEY]\"              #Ex: AVIAGI4VJJXF34LCFSN2\n",
    "AWS_SECRET_KEY = \"[ACCESS SECRET]\"              #Ex: 0Ps+NeWy1uf5cXfzg8qZoABdwv9oBbJh2Q0n2pB4\n",
    "AWS_REGION = \"[REGION]\"                         #Ex: us-east-1\n",
    "\n",
    "AWS_ENDPOINT = f\"s3.{AWS_REGION}.amazonaws.com\"\n",
    "S3_BUCKET = f\"s3a://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ae7dc25-1aa3-4d63-8b10-163f8d972896",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define DB2YB Function"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import re\n",
    "import glob\n",
    "import time\n",
    "import psycopg2\n",
    "import html\n",
    "import boto3\n",
    "\n",
    "def remove_s3_directory(bucket_name, prefix, aws_access_key_id, aws_secret_access_key, aws_region_name=\"us-east-1\"):\n",
    "    session = boto3.Session(\n",
    "        aws_access_key_id=aws_access_key_id,\n",
    "        aws_secret_access_key=aws_secret_access_key,\n",
    "    )\n",
    "    s3 = session.client('s3')\n",
    "    objects_to_delete = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "    delete_keys = {'Objects': [{'Key': obj['Key']} for obj in objects_to_delete.get('Contents', [])]}\n",
    "    if delete_keys['Objects']:\n",
    "        s3.delete_objects(Bucket=bucket_name, Delete=delete_keys)\n",
    "\n",
    "def configure_test_s3():\n",
    "    #raises an error if it fails\n",
    "    spark.conf.set(\"fs.s3a.access.key\", AWS_ACCESS_KEY_ID)\n",
    "    spark.conf.set(\"fs.s3a.secret.key\", AWS_SECRET_KEY)\n",
    "    spark.conf.set(\"fs.s3a.endpoint\", AWS_ENDPOINT)\n",
    "\n",
    "    data = [(3, 9), (4, 16)]\n",
    "    columns = [\"A\", \"B\"]\n",
    "    df = spark.createDataFrame(data, columns)\n",
    "\n",
    "    df.write \\\n",
    "        .format(\"csv\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .save(S3_BUCKET + \"/test.csv\")\n",
    "    df_read = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .load(S3_BUCKET + \"/test.csv\")\n",
    "    remove_s3_directory(BUCKET_NAME, \"/test.csv\", AWS_ACCESS_KEY_ID, AWS_SECRET_KEY, AWS_REGION)\n",
    "    \n",
    "\n",
    "def configure_test_yb(createS3): \n",
    "    with psycopg2.connect(host=YB_HOST, dbname=YB_DATABASE, user=YB_USER, password=YB_PASSWORD, application_name=f'DB2YB') as connYB:\n",
    "        with connYB.cursor() as cursor:\n",
    "            cursor.execute(f\"CREATE SCHEMA IF NOT EXISTS {YB_SCHEMA};\") \n",
    "            cursor.execute(f\"SET search_path TO {YB_SCHEMA};\")\n",
    "            if createS3:\n",
    "                cursor.execute(f'drop external location if exists \"dbcopy-loc\";')\n",
    "                cursor.execute(f'drop external format if exists \"dbcopy-fmt\";')\n",
    "                cursor.execute(f'drop external storage if exists \"dbcopy-store\";')\n",
    "                cursor.execute(f\"\"\"create external storage \"dbcopy-store\" type s3 \\\n",
    "                                    endpoint 'https://{AWS_ENDPOINT}' \\\n",
    "                                    region '{AWS_REGION}' \\\n",
    "                                    identity '{AWS_ACCESS_KEY_ID}' \\\n",
    "                                    credential '{AWS_SECRET_KEY}';\"\"\")\n",
    "                cursor.execute(f\"\"\"create external format \"dbcopy-fmt\" type csv with (delimiter '^', escape_char '\\\\', num_header_lines '1');\"\"\")\n",
    "                cursor.execute(f\"\"\"create external location \"dbcopy-loc\" path '{BUCKET_NAME}' \\\n",
    "                                    external storage \"dbcopy-store\" \\\n",
    "                                    external format \"dbcopy-fmt\";\"\"\")\n",
    "\n",
    "def lc(name):\n",
    "    if re.match(r'^\\d', name):\n",
    "        out_name = f'\"{name}\"'\n",
    "    elif name.upper() == name:\n",
    "        out_name = name.lower()\n",
    "    elif name.lower() == name:\n",
    "        out_name = name\n",
    "    else :\n",
    "        out_name = f'\"{name}\"'\n",
    "    return out_name\n",
    "\n",
    "def make_fq(table_name, schema, database=None):\n",
    "    if database is None:\n",
    "        return f'{lc(schema)}.{lc(table_name)}'\n",
    "    return f'{lc(database)}.{lc(schema)}.{lc(table_name)}'\n",
    "\n",
    "def get_matching_tables(regex_patterns):\n",
    "    df = spark.sql(f\"SHOW TABLES IN {DB_DATABASE}.{DB_SCHEMA}\")\n",
    "    tables = [make_fq(row[\"tableName\"], row[\"database\"], DB_DATABASE) for row in df.collect()] \n",
    "\n",
    "    matches = []\n",
    "    for pattern in regex_patterns:\n",
    "        name=make_fq(pattern,DB_SCHEMA,DB_DATABASE)\n",
    "        regex = re.compile(name)\n",
    "        matched = [table for table in tables if regex.fullmatch(table)]\n",
    "        matches.extend(matched)\n",
    "\n",
    "    return sorted(set(matches))\n",
    "\n",
    "\n",
    "def write_ddl_file(ddl, file_name):\n",
    "    start_time = time.time()\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.write(ddl)\n",
    "    print(f\"   * {(time.time()-start_time):7.2f}    - Wrote DDL to file {file_name}\")\n",
    "\n",
    "def read_ddl_file(file_name):\n",
    "    start_time = time.time()\n",
    "    with open(file_name, 'r') as f:\n",
    "        ddl = f.read()\n",
    "    print(f\"   * {(time.time()-start_time):7.2f}    - Read table DDL from file {file_name}\")\n",
    "    return ddl\n",
    "\n",
    "\n",
    "def getTableDDL(fq_tableName,yb_schema):\n",
    "    DB_TYPE_MAP = {\n",
    "            \"STRING\": \"VARCHAR(2000)\",\n",
    "            \"INT\": \"INTEGER\",\n",
    "            \"INTEGER\": \"INTEGER\",\n",
    "            \"BIGINT\": \"BIGINT\",\n",
    "            \"DOUBLE\": \"DOUBLE PRECISION\",\n",
    "            \"FLOAT\": \"REAL\",\n",
    "            \"DECIMAL\": \"NUMERIC\",\n",
    "            \"BOOLEAN\": \"BOOLEAN\",\n",
    "            \"TIMESTAMP\": \"TIMESTAMP\",\n",
    "            \"DATE\": \"DATE\",\n",
    "            \"BINARY\": \"BYTEA\",\n",
    "            \"ARRAY\": \"JSONB\",\n",
    "            \"MAP\": \"JSONB\",\n",
    "            \"STRUCT\": \"JSONB\"\n",
    "        }\n",
    "\n",
    "    def split_type(type_str):\n",
    "        pattern = r'^(\\w+)(\\([^)]+\\))?$'\n",
    "        match = re.match(pattern, type_str.strip())\n",
    "\n",
    "        if match:\n",
    "            raw_type = match.group(1)\n",
    "            precision = match.group(2) if match.group(2) is not None else \"\"\n",
    "            return raw_type, precision\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid type format: {type_str}\")\n",
    "\n",
    "    ddl_query = f\"DESCRIBE TABLE {fq_tableName}\"\n",
    "    ddl_df = spark.sql(ddl_query)\n",
    "\n",
    "    column_lines=[f\"{row['col_name']} {row['data_type']}\" for row in ddl_df.collect()]\n",
    "        \n",
    "    columns_sql = []\n",
    "    for line in column_lines:\n",
    "        if line.startswith(\"#\"): \n",
    "            break\n",
    "        parts = re.split(r'\\s+', line, 2)\n",
    "        col_name = parts[0].strip('`\"')\n",
    "        (raw_type,precision) = split_type(parts[1].upper())\n",
    "        yb_type = next((pg for db, pg in DB_TYPE_MAP.items() if raw_type.startswith(db)), \"VARCHAR\")\n",
    "        if precision: \n",
    "            columns_sql.append(f'  {lc(col_name)} {yb_type}{precision}')\n",
    "        elif yb_type == \"NUMERIC\":\n",
    "            columns_sql.append(f'  {lc(col_name)} {yb_type}(38,18)')\n",
    "        else:\n",
    "            columns_sql.append(f'  {lc(col_name)} {yb_type}')\n",
    "\n",
    "\n",
    "    yb_table_name = fq_tableName.split(\".\")[-1]\n",
    "    create_stmt = f'CREATE TABLE {lc(yb_schema)+\".\"+lc(yb_table_name)} (\\n' + \",\\n\".join(columns_sql) + \"\\n) distribute random;\"\n",
    "    return create_stmt\n",
    "\n",
    "def create_table_in_yellowbrick(ddl, table_name):\n",
    "    start_time=time.time()\n",
    "    try:\n",
    "        with psycopg2.connect(host=YB_HOST, dbname=YB_DATABASE, user=YB_USER, password=YB_PASSWORD, application_name=f'DB2YB') as connYB:\n",
    "            with connYB.cursor() as cursorYB:\n",
    "                print(f\"   * {(time.time()-start_time):7.2f} YB - Recreating {table_name}\")\n",
    "                cursorYB.execute(f'DROP TABLE IF EXISTS {make_fq(table_name, YB_SCHEMA, YB_DATABASE)};')\n",
    "                cursorYB.execute(ddl)\n",
    "            connYB.commit()\n",
    "        print(f\"   * {(time.time()-start_time):7.2f} YB - -- Dropped {table_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating table {table_name}: {e}\")\n",
    "    print(f\"   * {(time.time()-start_time):7.2f} YB - -- Created Table {table_name}\")\n",
    "\n",
    "def append_prep_destination(append, table):\n",
    "    start_time = time.time()\n",
    "    predicate = fix_column_names(html.unescape(append),True).replace('\\u00A0', ' ')\n",
    "    print(f\"   * {(time.time()-start_time):7.2f} YB - Append mode.  Deleting rows matching {predicate} from {table}\")\n",
    "    with psycopg2.connect(host=YB_HOST, dbname=YB_DATABASE, user=YB_USER, password=YB_PASSWORD, application_name=f'DB2YB') as connYB:\n",
    "        with connYB.cursor() as cursorYB:\n",
    "            delete_statement = f'DELETE FROM {table} WHERE {predicate};'\n",
    "            cursorYB.execute(delete_statement)\n",
    "            connYB.commit()\n",
    "    \n",
    "\n",
    "def fix_column_names(predicate,YB=True):\n",
    "    if YB:\n",
    "        output = re.sub(r'\\[', '\"', predicate)\n",
    "        output = re.sub(r'\\]', '\"', output)\n",
    "    else:\n",
    "        output = re.sub(r'\\[', '', predicate)\n",
    "        output = re.sub(r'\\]', '', output)\n",
    "    return output\n",
    "\n",
    "def get_data(full_table_name, limit, where, append):\n",
    "    sql_query=True\n",
    "    if append:\n",
    "        query = fix_column_names(html.unescape(f\"SELECT * FROM {full_table_name} WHERE {append}\"),False)\n",
    "    else:\n",
    "        if where:\n",
    "            query = fix_column_names(html.unescape(f\"SELECT * FROM {full_table_name} WHERE {where}\"),False)\n",
    "        else:\n",
    "            sql_query=False\n",
    "    \n",
    "    start_time=time.time()\n",
    "    if not sql_query: \n",
    "        if limit:\n",
    "            lText=f\"up to {limit} rows of\"\n",
    "        else:\n",
    "            lText=\"entire\"\n",
    "        print(f\"             DB - Loading {lText} Spark table\") \n",
    "        df=spark.table(full_table_name)\n",
    "        if limit:\n",
    "            df=df.limit(limit)\n",
    "    else: \n",
    "        if limit:\n",
    "            query += f\" LIMIT {limit}\"\n",
    "        print(f\"             DB - Executing {query}\") \n",
    "        df = spark.sql(query)\n",
    "    row_count = df.count()\n",
    "    print(f\"   * {(time.time()-start_time):7.2f} DB - Received {row_count} rows from table: {full_table_name}\")\n",
    "    return (row_count, df)\n",
    "\n",
    "def spark_write(df, yb_table_name):\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        df.write \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", f\"jdbc:postgresql://{YB_HOST}:5432/{YB_DATABASE}\") \\\n",
    "            .option(\"dbtable\", f\"{YB_SCHEMA}.{yb_table_name}\") \\\n",
    "            .option(\"currentSchema\", YB_SCHEMA) \\\n",
    "            .option(\"user\", YB_USER) \\\n",
    "            .option(\"password\", YB_PASSWORD) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .option(\"numPartitions\", SPARK_PARITIONS) \\\n",
    "            .mode(\"append\") \\\n",
    "            .save() \n",
    "        count=df.count()\n",
    "        print(f\"   * {(time.time()-start_time):7.2f} YB - Spark Wrote {count} rows to table {yb_table_name} {(count/(time.time()-start_time)):,.0f} RPS\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Spark write to {yb_table_name} failed: {e}\")\n",
    "        count=0\n",
    "    return count\n",
    "\n",
    "def s3_write(df, yb_table_name):\n",
    "\n",
    "    def export_table_to_csv(df, output_path, table_name):\n",
    "        start_time=time.time()\n",
    "        row_count = df.count()\n",
    "        df.repartition(SPARK_PARITIONS).write.csv(output_path, mode='overwrite', header=True, sep=\"^\", escape=\"\\\\\")\n",
    "        print(f\"   * {(time.time()-start_time):7.2f} DB - Exported {row_count} rows to CSV directory: {output_path}.\")\n",
    "        return row_count\n",
    "\n",
    "    def copy_to_yellowbrick(file_dir, table_name, rows):\n",
    "        all_start_time = time.time()\n",
    "        print(f\"             YB - Loading S3 files to {table_name}\")\n",
    "        with psycopg2.connect(host=YB_HOST, dbname=YB_DATABASE, user=YB_USER, password=YB_PASSWORD, application_name=f'DB2YB') as connYB:\n",
    "            with connYB.cursor() as copy_cursor:\n",
    "                try:\n",
    "                    start_time = time.time()\n",
    "                    copy_sql = f\"\"\"\n",
    "                        LOAD TABLE {make_fq(table_name, YB_SCHEMA, YB_DATABASE)} FROM ('{file_dir}/') EXTERNAL LOCATION \"dbcopy-loc\" EXTERNAL FORMAT \"dbcopy-fmt\" WITH (num_readers '{SPARK_PARITIONS}', read_sources_concurrently 'ALWAYS')\n",
    "                    \"\"\"\n",
    "                    copy_cursor.execute(copy_sql)   \n",
    "                    connYB.commit()\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ COPY failed: {e}\")\n",
    "                print(f\"   * {(time.time() - all_start_time):7.2f} YB - Copied all parts to {table_name}. {(rows/(time.time()-all_start_time)):,.0f} RPS\")\n",
    "\n",
    "    csv_dir_name = f\"{yb_table_name}.csv\"\n",
    "    rows = export_table_to_csv(df,S3_BUCKET+\"/\"+csv_dir_name, yb_table_name)\n",
    "    copy_to_yellowbrick(csv_dir_name, yb_table_name, rows)\n",
    "    remove_s3_directory(BUCKET_NAME, csv_dir_name+'/', AWS_ACCESS_KEY_ID, AWS_SECRET_KEY, AWS_REGION)\n",
    "    return rows\n",
    "\n",
    "\n",
    "def validate_table(table_name, rows, append_mode):\n",
    "    start_time=time.time()   \n",
    "    count = 0\n",
    "    with psycopg2.connect(host=YB_HOST, dbname=YB_DATABASE, user=YB_USER, password=YB_PASSWORD, application_name=f'DB2YB') as connYB:\n",
    "        with connYB.cursor() as cursorYB:\n",
    "            query = f'SELECT COUNT(*) FROM {make_fq(table_name, YB_SCHEMA, YB_DATABASE)};'\n",
    "            cursorYB.execute(query)\n",
    "            count = cursorYB.fetchone()[0]\n",
    "\n",
    "    if count == rows:\n",
    "        print(f\"   * {(time.time()-start_time):7.2f} YB - Validation successful: {count} rows in Yellowbrick table {table_name}\")\n",
    "    else:\n",
    "        if append_mode:\n",
    "            print(f\"   * {(time.time()-start_time):7.2f} YB - Total Rows after append: {count} rows in Yellowbrick table {table_name}\")\n",
    "        else:\n",
    "            print(f\"❌ Validation failed: Expected {rows} rows, but found {count} rows in Yellowbrick table {table_name}\")        \n",
    "\n",
    "    \n",
    "def DB2YB(table_patterns,\n",
    "          limit=None,\n",
    "          append=None,\n",
    "          write_ddl=None,\n",
    "          read_ddl=None,\n",
    "          where=None,\n",
    "          useS3=True\n",
    "          ):\n",
    "    \n",
    "    if append and where:\n",
    "        raise Exception(\"Cannot specify both append and where\")\n",
    "\n",
    "    if write_ddl and read_ddl:\n",
    "        raise Exception(\"Cannot specify both write_ddl and read_ddl\")\n",
    "\n",
    "    if useS3: \n",
    "        start_time=time.time()\n",
    "        print(\"S3 Acceleration Enabled - Testing connection: \", end=\"\")\n",
    "        configure_test_s3()\n",
    "        print(f\"{time.time()-start_time:.1f} seconds\")\n",
    "    else:\n",
    "        print(\"S3 Acceleration not enabled\")\n",
    "\n",
    "    print(f\"Connected to Databricks via Spark: Catalog={DB_DATABASE} Schema={DB_SCHEMA}\")\n",
    "    \n",
    "    print(f\"Connecting to Yellowbrick: {YB_DATABASE}, Schema: {YB_SCHEMA}: \", end=\"\")\n",
    "    start_time=time.time()\n",
    "    configure_test_yb(useS3)\n",
    "    print(f\"{time.time()-start_time:.1f} seconds\")\n",
    "\n",
    "    table_list = get_matching_tables(table_patterns)\n",
    "    print(f\"\\nFound {len(table_list)} tables matching the patterns:\")\n",
    "    for table in table_list:\n",
    "        print(f\" - {table}\")\n",
    "    print(\"\")\n",
    "\n",
    "    for table in table_list:\n",
    "        yb_table_name = table.split(\".\")[-1].strip('\"')\n",
    "        fq_yb_table_name =  make_fq(yb_table_name, YB_SCHEMA, YB_DATABASE) \n",
    "        start_time=time.time()\n",
    "        print(f\"Processing table {table} -> {fq_yb_table_name}\")\n",
    "        print(f\"        Secs DW - Event\")\n",
    "        print(f\"   =====================================================\")\n",
    "        if not append:\n",
    "            if read_ddl:\n",
    "                ddl = read_ddl_file(f\"{read_ddl}/{yb_table_name}.ddl\")\n",
    "            else:\n",
    "                ddl = getTableDDL(table, YB_SCHEMA)\n",
    "                print(f\"   * {(time.time()-start_time):7.2f} DB - Getting DDL for {table}\")\n",
    "            if write_ddl:\n",
    "                write_ddl_file(ddl, f\"{write_ddl}/{yb_table_name}.ddl\")\n",
    "                print()\n",
    "                continue\n",
    "            create_table_in_yellowbrick(ddl,yb_table_name)\n",
    "        else:\n",
    "            append_prep_destination(append, fq_yb_table_name)\n",
    "\n",
    "        (rows, df) = get_data(table, limit, where, append)\n",
    "        if rows==0:\n",
    "            print(f\"   * {(time.time()-start_time):7.2f}    - Nothing to process - No rows returned.\")\n",
    "        else:\n",
    "            if not useS3 or rows<MIN_S3_ROWS:\n",
    "                if useS3:\n",
    "                    print(f\"   * {(time.time()-start_time):7.2f} YB - Small table, using spark dataframe write\")\n",
    "                rows = spark_write(df, yb_table_name)\n",
    "            else:\n",
    "                rows = s3_write(df, yb_table_name)\n",
    "            validate_table(yb_table_name, rows, append)\n",
    "            print(f\"   * {(time.time()-start_time):7.2f}    - Finished processing table {table} -> {fq_yb_table_name}\")\n",
    "    \n",
    "        print()\n",
    "\n",
    "    print(\"All tables processed successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9017be91-802c-4f3b-b99e-be50f4ee9736",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Copy All Data from All Tables"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Acceleration Enabled - Testing connection: 2.8 seconds\nConnected to Databricks via Spark: Catalog=ybaws Schema=tpcds\nConnecting to Yellowbrick: ms_test, Schema: tpcds: 0.2 seconds\n\nFound 25 tables matching the patterns:\n - ybaws.tpcds.call_center\n - ybaws.tpcds.catalog_page\n - ybaws.tpcds.catalog_returns\n - ybaws.tpcds.catalog_sales\n - ybaws.tpcds.customer\n - ybaws.tpcds.customer_address\n - ybaws.tpcds.customer_demographics\n - ybaws.tpcds.date_dim\n - ybaws.tpcds.dbgen_version\n - ybaws.tpcds.household_demographics\n - ybaws.tpcds.income_band\n - ybaws.tpcds.inventory\n - ybaws.tpcds.item\n - ybaws.tpcds.promotion\n - ybaws.tpcds.reason\n - ybaws.tpcds.ship_mode\n - ybaws.tpcds.store\n - ybaws.tpcds.store_returns\n - ybaws.tpcds.store_sales\n - ybaws.tpcds.time_dim\n - ybaws.tpcds.warehouse\n - ybaws.tpcds.web_page\n - ybaws.tpcds.web_returns\n - ybaws.tpcds.web_sales\n - ybaws.tpcds.web_site\n\nProcessing table ybaws.tpcds.call_center -> ms_test.tpcds.call_center\n        Secs DW - Event\n   =====================================================\n   *    0.32 DB - Getting DDL for ybaws.tpcds.call_center\n   *    0.02 YB - Recreating call_center\n   *    0.09 YB - -- Dropped call_center\n   *    0.09 YB - -- Created Table call_center\n             DB - Loading entire Spark table\n   *    0.30 DB - Received 30 rows from table: ybaws.tpcds.call_center\n   *    0.70 YB - Small table, using spark dataframe write\n   *    0.59 YB - Spark Wrote 30 rows to table call_center 51 RPS\n   *    0.16 YB - Validation successful: 30 rows in Yellowbrick table call_center\n   *    1.45    - Finished processing table ybaws.tpcds.call_center -> ms_test.tpcds.call_center\n\nProcessing table ybaws.tpcds.catalog_page -> ms_test.tpcds.catalog_page\n        Secs DW - Event\n   =====================================================\n   *    0.20 DB - Getting DDL for ybaws.tpcds.catalog_page\n   *    0.02 YB - Recreating catalog_page\n   *    0.08 YB - -- Dropped catalog_page\n   *    0.08 YB - -- Created Table catalog_page\n             DB - Loading entire Spark table\n   *    0.29 DB - Received 20400 rows from table: ybaws.tpcds.catalog_page\n   *    0.57 YB - Small table, using spark dataframe write\n   *    1.64 YB - Spark Wrote 20400 rows to table catalog_page 12,454 RPS\n   *    0.16 YB - Validation successful: 20400 rows in Yellowbrick table catalog_page\n   *    2.36    - Finished processing table ybaws.tpcds.catalog_page -> ms_test.tpcds.catalog_page\n\nProcessing table ybaws.tpcds.catalog_returns -> ms_test.tpcds.catalog_returns\n        Secs DW - Event\n   =====================================================\n   *    0.21 DB - Getting DDL for ybaws.tpcds.catalog_returns\n   *    0.02 YB - Recreating catalog_returns\n   *    0.08 YB - -- Dropped catalog_returns\n   *    0.08 YB - -- Created Table catalog_returns\n             DB - Loading entire Spark table\n   *    0.30 DB - Received 14398689 rows from table: ybaws.tpcds.catalog_returns\n   *    5.46 DB - Exported 14398689 rows to CSV directory: s3a://ms-dbtest/catalog_returns.csv.\n             YB - Loading S3 files to catalog_returns\n   *   10.38 YB - Copied all parts to catalog_returns. 1,387,110 RPS\n   *    0.17 YB - Validation successful: 14398689 rows in Yellowbrick table catalog_returns\n   *   17.37    - Finished processing table ybaws.tpcds.catalog_returns -> ms_test.tpcds.catalog_returns\n\nProcessing table ybaws.tpcds.catalog_sales -> ms_test.tpcds.catalog_sales\n        Secs DW - Event\n   =====================================================\n   *    0.17 DB - Getting DDL for ybaws.tpcds.catalog_sales\n   *    0.02 YB - Recreating catalog_sales\n   *    0.08 YB - -- Dropped catalog_sales\n   *    0.08 YB - -- Created Table catalog_sales\n             DB - Loading entire Spark table\n   *    0.59 DB - Received 143978000 rows from table: ybaws.tpcds.catalog_sales\n   *   47.66 DB - Exported 143978000 rows to CSV directory: s3a://ms-dbtest/catalog_sales.csv.\n             YB - Loading S3 files to catalog_sales\n   *   65.01 YB - Copied all parts to catalog_sales. 2,214,733 RPS\n   *    0.14 YB - Validation successful: 143978000 rows in Yellowbrick table catalog_sales\n   *  114.39    - Finished processing table ybaws.tpcds.catalog_sales -> ms_test.tpcds.catalog_sales\n\nProcessing table ybaws.tpcds.customer -> ms_test.tpcds.customer\n        Secs DW - Event\n   =====================================================\n   *    0.29 DB - Getting DDL for ybaws.tpcds.customer\n   *    0.02 YB - Recreating customer\n   *    0.08 YB - -- Dropped customer\n   *    0.08 YB - -- Created Table customer\n             DB - Loading entire Spark table\n   *    0.29 DB - Received 2000000 rows from table: ybaws.tpcds.customer\n   *    3.99 DB - Exported 2000000 rows to CSV directory: s3a://ms-dbtest/customer.csv.\n             YB - Loading S3 files to customer\n   *    6.72 YB - Copied all parts to customer. 297,604 RPS\n   *    0.14 YB - Validation successful: 2000000 rows in Yellowbrick table customer\n   *   12.38    - Finished processing table ybaws.tpcds.customer -> ms_test.tpcds.customer\n\nProcessing table ybaws.tpcds.customer_address -> ms_test.tpcds.customer_address\n        Secs DW - Event\n   =====================================================\n   *    0.23 DB - Getting DDL for ybaws.tpcds.customer_address\n   *    0.02 YB - Recreating customer_address\n   *    0.09 YB - -- Dropped customer_address\n   *    0.09 YB - -- Created Table customer_address\n             DB - Loading entire Spark table\n   *    0.28 DB - Received 1000000 rows from table: ybaws.tpcds.customer_address\n   *    2.15 DB - Exported 1000000 rows to CSV directory: s3a://ms-dbtest/customer_address.csv.\n             YB - Loading S3 files to customer_address\n   *    5.67 YB - Copied all parts to customer_address. 176,308 RPS\n   *    0.14 YB - Validation successful: 1000000 rows in Yellowbrick table customer_address\n   *    9.05    - Finished processing table ybaws.tpcds.customer_address -> ms_test.tpcds.customer_address\n\nProcessing table ybaws.tpcds.customer_demographics -> ms_test.tpcds.customer_demographics\n        Secs DW - Event\n   =====================================================\n   *    0.17 DB - Getting DDL for ybaws.tpcds.customer_demographics\n   *    0.02 YB - Recreating customer_demographics\n   *    0.09 YB - -- Dropped customer_demographics\n   *    0.09 YB - -- Created Table customer_demographics\n             DB - Loading entire Spark table\n   *    0.31 DB - Received 1920800 rows from table: ybaws.tpcds.customer_demographics\n   *    1.61 DB - Exported 1920800 rows to CSV directory: s3a://ms-dbtest/customer_demographics.csv.\n             YB - Loading S3 files to customer_demographics\n   *    5.78 YB - Copied all parts to customer_demographics. 332,234 RPS\n   *    0.14 YB - Validation successful: 1920800 rows in Yellowbrick table customer_demographics\n   *    8.65    - Finished processing table ybaws.tpcds.customer_demographics -> ms_test.tpcds.customer_demographics\n\nProcessing table ybaws.tpcds.date_dim -> ms_test.tpcds.date_dim\n        Secs DW - Event\n   =====================================================\n   *    0.23 DB - Getting DDL for ybaws.tpcds.date_dim\n   *    0.02 YB - Recreating date_dim\n   *    0.08 YB - -- Dropped date_dim\n   *    0.08 YB - -- Created Table date_dim\n             DB - Loading entire Spark table\n   *    0.23 DB - Received 73049 rows from table: ybaws.tpcds.date_dim\n   *    0.54 YB - Small table, using spark dataframe write\n   *    7.01 YB - Spark Wrote 73049 rows to table date_dim 10,419 RPS\n   *    0.16 YB - Validation successful: 73049 rows in Yellowbrick table date_dim\n   *    7.71    - Finished processing table ybaws.tpcds.date_dim -> ms_test.tpcds.date_dim\n\nProcessing table ybaws.tpcds.dbgen_version -> ms_test.tpcds.dbgen_version\n        Secs DW - Event\n   =====================================================\n   *    0.20 DB - Getting DDL for ybaws.tpcds.dbgen_version\n   *    0.02 YB - Recreating dbgen_version\n   *    0.08 YB - -- Dropped dbgen_version\n   *    0.08 YB - -- Created Table dbgen_version\n             DB - Loading entire Spark table\n   *    0.25 DB - Received 1 rows from table: ybaws.tpcds.dbgen_version\n   *    0.53 YB - Small table, using spark dataframe write\n   *    0.61 YB - Spark Wrote 1 rows to table dbgen_version 2 RPS\n   *    0.16 YB - Validation successful: 1 rows in Yellowbrick table dbgen_version\n   *    1.30    - Finished processing table ybaws.tpcds.dbgen_version -> ms_test.tpcds.dbgen_version\n\nProcessing table ybaws.tpcds.household_demographics -> ms_test.tpcds.household_demographics\n        Secs DW - Event\n   =====================================================\n   *    0.21 DB - Getting DDL for ybaws.tpcds.household_demographics\n   *    0.02 YB - Recreating household_demographics\n   *    0.08 YB - -- Dropped household_demographics\n   *    0.08 YB - -- Created Table household_demographics\n             DB - Loading entire Spark table\n   *    0.25 DB - Received 7200 rows from table: ybaws.tpcds.household_demographics\n   *    0.54 YB - Small table, using spark dataframe write\n   *    0.90 YB - Spark Wrote 7200 rows to table household_demographics 7,998 RPS\n   *    0.16 YB - Validation successful: 7200 rows in Yellowbrick table household_demographics\n   *    1.59    - Finished processing table ybaws.tpcds.household_demographics -> ms_test.tpcds.household_demographics\n\nProcessing table ybaws.tpcds.income_band -> ms_test.tpcds.income_band\n        Secs DW - Event\n   =====================================================\n   *    0.17 DB - Getting DDL for ybaws.tpcds.income_band\n   *    0.02 YB - Recreating income_band\n   *    0.07 YB - -- Dropped income_band\n   *    0.07 YB - -- Created Table income_band\n             DB - Loading entire Spark table\n   *    0.31 DB - Received 20 rows from table: ybaws.tpcds.income_band\n   *    0.54 YB - Small table, using spark dataframe write\n   *    0.62 YB - Spark Wrote 20 rows to table income_band 32 RPS\n   *    0.15 YB - Validation successful: 20 rows in Yellowbrick table income_band\n   *    1.32    - Finished processing table ybaws.tpcds.income_band -> ms_test.tpcds.income_band\n\nProcessing table ybaws.tpcds.inventory -> ms_test.tpcds.inventory\n        Secs DW - Event\n   =====================================================\n   *    0.35 DB - Getting DDL for ybaws.tpcds.inventory\n   *    0.02 YB - Recreating inventory\n   *    0.08 YB - -- Dropped inventory\n   *    0.08 YB - -- Created Table inventory\n             DB - Loading entire Spark table\n   *    0.26 DB - Received 399330000 rows from table: ybaws.tpcds.inventory\n   *   18.79 DB - Exported 399330000 rows to CSV directory: s3a://ms-dbtest/inventory.csv.\n             YB - Loading S3 files to inventory\n   *   30.62 YB - Copied all parts to inventory. 13,043,118 RPS\n   *    0.14 YB - Validation successful: 399330000 rows in Yellowbrick table inventory\n   *   50.94    - Finished processing table ybaws.tpcds.inventory -> ms_test.tpcds.inventory\n\nProcessing table ybaws.tpcds.item -> ms_test.tpcds.item\n        Secs DW - Event\n   =====================================================\n   *    0.51 DB - Getting DDL for ybaws.tpcds.item\n   *    0.02 YB - Recreating item\n   *    0.08 YB - -- Dropped item\n   *    0.08 YB - -- Created Table item\n             DB - Loading entire Spark table\n   *    0.38 DB - Received 204000 rows from table: ybaws.tpcds.item\n   *    0.97 YB - Small table, using spark dataframe write\n   *   19.26 YB - Spark Wrote 204000 rows to table item 10,593 RPS\n   *    0.17 YB - Validation successful: 204000 rows in Yellowbrick table item\n   *   20.40    - Finished processing table ybaws.tpcds.item -> ms_test.tpcds.item\n\nProcessing table ybaws.tpcds.promotion -> ms_test.tpcds.promotion\n        Secs DW - Event\n   =====================================================\n   *    0.19 DB - Getting DDL for ybaws.tpcds.promotion\n   *    0.02 YB - Recreating promotion\n   *    0.08 YB - -- Dropped promotion\n   *    0.08 YB - -- Created Table promotion\n             DB - Loading entire Spark table\n   *    0.26 DB - Received 1000 rows from table: ybaws.tpcds.promotion\n   *    0.52 YB - Small table, using spark dataframe write\n   *    0.69 YB - Spark Wrote 1000 rows to table promotion 1,443 RPS\n   *    0.16 YB - Validation successful: 1000 rows in Yellowbrick table promotion\n   *    1.38    - Finished processing table ybaws.tpcds.promotion -> ms_test.tpcds.promotion\n\nProcessing table ybaws.tpcds.reason -> ms_test.tpcds.reason\n        Secs DW - Event\n   =====================================================\n   *    0.21 DB - Getting DDL for ybaws.tpcds.reason\n   *    0.02 YB - Recreating reason\n   *    0.09 YB - -- Dropped reason\n   *    0.09 YB - -- Created Table reason\n             DB - Loading entire Spark table\n   *    0.32 DB - Received 55 rows from table: ybaws.tpcds.reason\n   *    0.62 YB - Small table, using spark dataframe write\n   *    0.54 YB - Spark Wrote 55 rows to table reason 102 RPS\n   *    0.15 YB - Validation successful: 55 rows in Yellowbrick table reason\n   *    1.32    - Finished processing table ybaws.tpcds.reason -> ms_test.tpcds.reason\n\nProcessing table ybaws.tpcds.ship_mode -> ms_test.tpcds.ship_mode\n        Secs DW - Event\n   =====================================================\n   *    0.17 DB - Getting DDL for ybaws.tpcds.ship_mode\n   *    0.02 YB - Recreating ship_mode\n   *    0.09 YB - -- Dropped ship_mode\n   *    0.09 YB - -- Created Table ship_mode\n             DB - Loading entire Spark table\n   *    0.25 DB - Received 20 rows from table: ybaws.tpcds.ship_mode\n   *    0.51 YB - Small table, using spark dataframe write\n   *    0.59 YB - Spark Wrote 20 rows to table ship_mode 34 RPS\n   *    0.15 YB - Validation successful: 20 rows in Yellowbrick table ship_mode\n   *    1.26    - Finished processing table ybaws.tpcds.ship_mode -> ms_test.tpcds.ship_mode\n\nProcessing table ybaws.tpcds.store -> ms_test.tpcds.store\n        Secs DW - Event\n   =====================================================\n   *    0.16 DB - Getting DDL for ybaws.tpcds.store\n   *    0.02 YB - Recreating store\n   *    0.08 YB - -- Dropped store\n   *    0.08 YB - -- Created Table store\n             DB - Loading entire Spark table\n   *    0.23 DB - Received 402 rows from table: ybaws.tpcds.store\n   *    0.47 YB - Small table, using spark dataframe write\n   *    0.72 YB - Spark Wrote 402 rows to table store 558 RPS\n   *    0.16 YB - Validation successful: 402 rows in Yellowbrick table store\n   *    1.35    - Finished processing table ybaws.tpcds.store -> ms_test.tpcds.store\n\nProcessing table ybaws.tpcds.store_returns -> ms_test.tpcds.store_returns\n        Secs DW - Event\n   =====================================================\n   *    0.22 DB - Getting DDL for ybaws.tpcds.store_returns\n   *    0.02 YB - Recreating store_returns\n   *    0.09 YB - -- Dropped store_returns\n   *    0.09 YB - -- Created Table store_returns\n             DB - Loading entire Spark table\n   *    0.25 DB - Received 28807165 rows from table: ybaws.tpcds.store_returns\n   *    8.17 DB - Exported 28807165 rows to CSV directory: s3a://ms-dbtest/store_returns.csv.\n             YB - Loading S3 files to store_returns\n   *   13.63 YB - Copied all parts to store_returns. 2,113,451 RPS\n   *    0.15 YB - Validation successful: 28807165 rows in Yellowbrick table store_returns\n   *   23.11    - Finished processing table ybaws.tpcds.store_returns -> ms_test.tpcds.store_returns\n\nProcessing table ybaws.tpcds.store_sales -> ms_test.tpcds.store_sales\n        Secs DW - Event\n   =====================================================\n   *    0.37 DB - Getting DDL for ybaws.tpcds.store_sales\n   *    0.02 YB - Recreating store_sales\n   *    0.08 YB - -- Dropped store_sales\n   *    0.08 YB - -- Created Table store_sales\n             DB - Loading entire Spark table\n   *    0.97 DB - Received 288035441 rows from table: ybaws.tpcds.store_sales\n   *   67.09 DB - Exported 288035441 rows to CSV directory: s3a://ms-dbtest/store_sales.csv.\n             YB - Loading S3 files to store_sales\n   *   87.06 YB - Copied all parts to store_sales. 3,308,341 RPS\n   *    0.15 YB - Validation successful: 288035441 rows in Yellowbrick table store_sales\n   *  156.36    - Finished processing table ybaws.tpcds.store_sales -> ms_test.tpcds.store_sales\n\nProcessing table ybaws.tpcds.time_dim -> ms_test.tpcds.time_dim\n        Secs DW - Event\n   =====================================================\n   *    0.25 DB - Getting DDL for ybaws.tpcds.time_dim\n   *    0.03 YB - Recreating time_dim\n   *    0.09 YB - -- Dropped time_dim\n   *    0.09 YB - -- Created Table time_dim\n             DB - Loading entire Spark table\n   *    0.35 DB - Received 86400 rows from table: ybaws.tpcds.time_dim\n   *    0.70 YB - Small table, using spark dataframe write\n   *    4.98 YB - Spark Wrote 86400 rows to table time_dim 17,338 RPS\n   *    0.16 YB - Validation successful: 86400 rows in Yellowbrick table time_dim\n   *    5.84    - Finished processing table ybaws.tpcds.time_dim -> ms_test.tpcds.time_dim\n\nProcessing table ybaws.tpcds.warehouse -> ms_test.tpcds.warehouse\n        Secs DW - Event\n   =====================================================\n   *    0.26 DB - Getting DDL for ybaws.tpcds.warehouse\n   *    0.02 YB - Recreating warehouse\n   *    0.09 YB - -- Dropped warehouse\n   *    0.09 YB - -- Created Table warehouse\n             DB - Loading entire Spark table\n   *    0.26 DB - Received 15 rows from table: ybaws.tpcds.warehouse\n   *    0.61 YB - Small table, using spark dataframe write\n   *    0.61 YB - Spark Wrote 15 rows to table warehouse 24 RPS\n   *    0.16 YB - Validation successful: 15 rows in Yellowbrick table warehouse\n   *    1.38    - Finished processing table ybaws.tpcds.warehouse -> ms_test.tpcds.warehouse\n\nProcessing table ybaws.tpcds.web_page -> ms_test.tpcds.web_page\n        Secs DW - Event\n   =====================================================\n   *    0.19 DB - Getting DDL for ybaws.tpcds.web_page\n   *    0.02 YB - Recreating web_page\n   *    0.08 YB - -- Dropped web_page\n   *    0.08 YB - -- Created Table web_page\n             DB - Loading entire Spark table\n   *    0.27 DB - Received 2040 rows from table: ybaws.tpcds.web_page\n   *    0.54 YB - Small table, using spark dataframe write\n   *    0.77 YB - Spark Wrote 2040 rows to table web_page 2,658 RPS\n   *    0.15 YB - Validation successful: 2040 rows in Yellowbrick table web_page\n   *    1.46    - Finished processing table ybaws.tpcds.web_page -> ms_test.tpcds.web_page\n\nProcessing table ybaws.tpcds.web_returns -> ms_test.tpcds.web_returns\n        Secs DW - Event\n   =====================================================\n   *    0.17 DB - Getting DDL for ybaws.tpcds.web_returns\n   *    0.02 YB - Recreating web_returns\n   *    0.08 YB - -- Dropped web_returns\n   *    0.08 YB - -- Created Table web_returns\n             DB - Loading entire Spark table\n   *    0.27 DB - Received 7200414 rows from table: ybaws.tpcds.web_returns\n   *    3.86 DB - Exported 7200414 rows to CSV directory: s3a://ms-dbtest/web_returns.csv.\n             YB - Loading S3 files to web_returns\n   *    8.39 YB - Copied all parts to web_returns. 858,492 RPS\n   *    0.14 YB - Validation successful: 7200414 rows in Yellowbrick table web_returns\n   *   13.86    - Finished processing table ybaws.tpcds.web_returns -> ms_test.tpcds.web_returns\n\nProcessing table ybaws.tpcds.web_sales -> ms_test.tpcds.web_sales\n        Secs DW - Event\n   =====================================================\n   *    0.24 DB - Getting DDL for ybaws.tpcds.web_sales\n   *    0.03 YB - Recreating web_sales\n   *    0.09 YB - -- Dropped web_sales\n   *    0.09 YB - -- Created Table web_sales\n             DB - Loading entire Spark table\n   *    0.41 DB - Received 72005755 rows from table: ybaws.tpcds.web_sales\n   *   24.58 DB - Exported 72005755 rows to CSV directory: s3a://ms-dbtest/web_sales.csv.\n             YB - Loading S3 files to web_sales\n   *   36.75 YB - Copied all parts to web_sales. 1,959,442 RPS\n   *    0.14 YB - Validation successful: 72005755 rows in Yellowbrick table web_sales\n   *   63.09    - Finished processing table ybaws.tpcds.web_sales -> ms_test.tpcds.web_sales\n\nProcessing table ybaws.tpcds.web_site -> ms_test.tpcds.web_site\n        Secs DW - Event\n   =====================================================\n   *    0.21 DB - Getting DDL for ybaws.tpcds.web_site\n   *    0.02 YB - Recreating web_site\n   *    0.09 YB - -- Dropped web_site\n   *    0.09 YB - -- Created Table web_site\n             DB - Loading entire Spark table\n   *    0.26 DB - Received 24 rows from table: ybaws.tpcds.web_site\n   *    0.56 YB - Small table, using spark dataframe write\n   *    0.62 YB - Spark Wrote 24 rows to table web_site 38 RPS\n   *    0.16 YB - Validation successful: 24 rows in Yellowbrick table web_site\n   *    1.34    - Finished processing table ybaws.tpcds.web_site -> ms_test.tpcds.web_site\n\nAll tables processed successfully.\n"
     ]
    }
   ],
   "source": [
    "DB2YB(table_patterns = ['.*'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ba015b5-5b0f-4c72-b68e-329a3cbf1752",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Copy call_center where cc_call_center_sk<20.   Limit it to 10 rows."
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Acceleration Enabled - Testing connection: 2.0 seconds\nConnected to Databricks via Spark: Catalog=ybaws Schema=tpcds\nConnecting to Yellowbrick: ms_test, Schema: tpcds: 0.1 seconds\n\nFound 1 tables matching the patterns:\n - ybaws.tpcds.call_center\n\nProcessing table ybaws.tpcds.call_center -> ms_test.tpcds.call_center\n        Secs DW - Event\n   =====================================================\n   *    0.20 DB - Getting DDL for ybaws.tpcds.call_center\n   *    0.02 YB - Recreating call_center\n   *    0.09 YB - -- Dropped call_center\n   *    0.09 YB - -- Created Table call_center\n             DB - Executing SELECT * FROM ybaws.tpcds.call_center WHERE cc_call_center_sk<20 LIMIT 10\n   *    0.35 DB - Received 10 rows from table: ybaws.tpcds.call_center\n   *    0.63 YB - Small table, using spark dataframe write\n   *    1.02 YB - Spark Wrote 10 rows to table call_center 10 RPS\n   *    0.26 YB - Validation successful: 10 rows in Yellowbrick table call_center\n   *    1.91    - Finished processing table ybaws.tpcds.call_center -> ms_test.tpcds.call_center\n\nAll tables processed successfully.\n"
     ]
    }
   ],
   "source": [
    "DB2YB(table_patterns = ['call_center'], where = '[cc_call_center_sk]<20', limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c8db37b-806c-4d9f-83f7-b50bc4547849",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Append call_center rows where cc_call_center_sk>=20"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Acceleration Enabled - Testing connection: 2.2 seconds\nConnected to Databricks via Spark: Catalog=ybaws Schema=tpcds\nConnecting to Yellowbrick: ms_test, Schema: tpcds: 0.2 seconds\n\nFound 1 tables matching the patterns:\n - ybaws.tpcds.call_center\n\nProcessing table ybaws.tpcds.call_center -> ms_test.tpcds.call_center\n        Secs DW - Event\n   =====================================================\n   *    0.00 YB - Append mode.  Deleting rows matching \"cc_call_center_sk\">=20 from ms_test.tpcds.call_center\n             DB - Executing SELECT * FROM ybaws.tpcds.call_center WHERE cc_call_center_sk>=20\n   *    0.36 DB - Received 11 rows from table: ybaws.tpcds.call_center\n   *    0.90 YB - Small table, using spark dataframe write\n   *    0.72 YB - Spark Wrote 11 rows to table call_center 15 RPS\n   *    0.25 YB - Total Rows after append: 21 rows in Yellowbrick table call_center\n   *    1.87    - Finished processing table ybaws.tpcds.call_center -> ms_test.tpcds.call_center\n\nAll tables processed successfully.\n"
     ]
    }
   ],
   "source": [
    "DB2YB(table_patterns = ['call_center'], append = '[cc_call_center_sk]>=20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f7ff299-76e8-45c6-b47b-27fd645e39e0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Grab the ddl for all tables matching store* and catalog*.   The DDL can then be edited by hand."
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Acceleration Enabled - Testing connection: 1.9 seconds\nConnected to Databricks via Spark: Catalog=ybaws Schema=tpcds\nConnecting to Yellowbrick: ms_test, Schema: tpcds: 0.1 seconds\n\nFound 6 tables matching the patterns:\n - ybaws.tpcds.catalog_page\n - ybaws.tpcds.catalog_returns\n - ybaws.tpcds.catalog_sales\n - ybaws.tpcds.store\n - ybaws.tpcds.store_returns\n - ybaws.tpcds.store_sales\n\nProcessing table ybaws.tpcds.catalog_page -> ms_test.tpcds.catalog_page\n        Secs DW - Event\n   =====================================================\n   *    0.22 DB - Getting DDL for ybaws.tpcds.catalog_page\n   *    0.56    - Wrote DDL to file ./ddl/catalog_page.ddl\n\nProcessing table ybaws.tpcds.catalog_returns -> ms_test.tpcds.catalog_returns\n        Secs DW - Event\n   =====================================================\n   *    0.20 DB - Getting DDL for ybaws.tpcds.catalog_returns\n   *    0.47    - Wrote DDL to file ./ddl/catalog_returns.ddl\n\nProcessing table ybaws.tpcds.catalog_sales -> ms_test.tpcds.catalog_sales\n        Secs DW - Event\n   =====================================================\n   *    0.19 DB - Getting DDL for ybaws.tpcds.catalog_sales\n   *    0.46    - Wrote DDL to file ./ddl/catalog_sales.ddl\n\nProcessing table ybaws.tpcds.store -> ms_test.tpcds.store\n        Secs DW - Event\n   =====================================================\n   *    0.17 DB - Getting DDL for ybaws.tpcds.store\n   *    0.34    - Wrote DDL to file ./ddl/store.ddl\n\nProcessing table ybaws.tpcds.store_returns -> ms_test.tpcds.store_returns\n        Secs DW - Event\n   =====================================================\n   *    0.14 DB - Getting DDL for ybaws.tpcds.store_returns\n   *    0.36    - Wrote DDL to file ./ddl/store_returns.ddl\n\nProcessing table ybaws.tpcds.store_sales -> ms_test.tpcds.store_sales\n        Secs DW - Event\n   =====================================================\n   *    0.29 DB - Getting DDL for ybaws.tpcds.store_sales\n   *    0.20    - Wrote DDL to file ./ddl/store_sales.ddl\n\nAll tables processed successfully.\n"
     ]
    }
   ],
   "source": [
    "DB2YB(table_patterns = ['store.*', 'catalog.*'],  write_ddl = './ddl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d3f0135-be0e-4171-8972-480321a0fd90",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Use the ddl definitions to copy store* and catalog* tables.  Limit rows copied to 2M"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Acceleration Enabled - Testing connection: 2.2 seconds\nConnected to Databricks via Spark: Catalog=ybaws Schema=tpcds\nConnecting to Yellowbrick: ms_test, Schema: tpcds: 0.1 seconds\n\nFound 6 tables matching the patterns:\n - ybaws.tpcds.catalog_page\n - ybaws.tpcds.catalog_returns\n - ybaws.tpcds.catalog_sales\n - ybaws.tpcds.store\n - ybaws.tpcds.store_returns\n - ybaws.tpcds.store_sales\n\nProcessing table ybaws.tpcds.catalog_page -> ms_test.tpcds.catalog_page\n        Secs DW - Event\n   =====================================================\n   *    0.23    - Read table DDL from file ./ddl/catalog_page.ddl\n   *    0.02 YB - Recreating catalog_page\n   *    0.08 YB - -- Dropped catalog_page\n   *    0.08 YB - -- Created Table catalog_page\n             DB - Loading up to 2000000 rows of Spark table\n   *    0.56 DB - Received 20400 rows from table: ybaws.tpcds.catalog_page\n   *    0.87 YB - Small table, using spark dataframe write\n   *    2.02 YB - Spark Wrote 20400 rows to table catalog_page 10,101 RPS\n   *    0.25 YB - Validation successful: 20400 rows in Yellowbrick table catalog_page\n   *    3.14    - Finished processing table ybaws.tpcds.catalog_page -> ms_test.tpcds.catalog_page\n\nProcessing table ybaws.tpcds.catalog_returns -> ms_test.tpcds.catalog_returns\n        Secs DW - Event\n   =====================================================\n   *    0.11    - Read table DDL from file ./ddl/catalog_returns.ddl\n   *    0.02 YB - Recreating catalog_returns\n   *    0.08 YB - -- Dropped catalog_returns\n   *    0.08 YB - -- Created Table catalog_returns\n             DB - Loading up to 2000000 rows of Spark table\n   *    0.42 DB - Received 2000000 rows from table: ybaws.tpcds.catalog_returns\n   *    4.51 DB - Exported 2000000 rows to CSV directory: s3a://ms-dbtest/catalog_returns.csv.\n             YB - Loading S3 files to catalog_returns\n   *    6.86 YB - Copied all parts to catalog_returns. 291,618 RPS\n   *    0.24 YB - Validation successful: 2000000 rows in Yellowbrick table catalog_returns\n   *   12.91    - Finished processing table ybaws.tpcds.catalog_returns -> ms_test.tpcds.catalog_returns\n\nProcessing table ybaws.tpcds.catalog_sales -> ms_test.tpcds.catalog_sales\n        Secs DW - Event\n   =====================================================\n   *    0.08    - Read table DDL from file ./ddl/catalog_sales.ddl\n   *    0.02 YB - Recreating catalog_sales\n   *    0.09 YB - -- Dropped catalog_sales\n   *    0.09 YB - -- Created Table catalog_sales\n             DB - Loading up to 2000000 rows of Spark table\n   *    0.38 DB - Received 2000000 rows from table: ybaws.tpcds.catalog_sales\n   *    5.69 DB - Exported 2000000 rows to CSV directory: s3a://ms-dbtest/catalog_sales.csv.\n             YB - Loading S3 files to catalog_sales\n   *    7.55 YB - Copied all parts to catalog_sales. 264,964 RPS\n   *    0.24 YB - Validation successful: 2000000 rows in Yellowbrick table catalog_sales\n   *   14.73    - Finished processing table ybaws.tpcds.catalog_sales -> ms_test.tpcds.catalog_sales\n\nProcessing table ybaws.tpcds.store -> ms_test.tpcds.store\n        Secs DW - Event\n   =====================================================\n   *    0.08    - Read table DDL from file ./ddl/store.ddl\n   *    0.03 YB - Recreating store\n   *    0.08 YB - -- Dropped store\n   *    0.08 YB - -- Created Table store\n             DB - Loading up to 2000000 rows of Spark table\n   *    0.51 DB - Received 402 rows from table: ybaws.tpcds.store\n   *    0.67 YB - Small table, using spark dataframe write\n   *    1.11 YB - Spark Wrote 402 rows to table store 363 RPS\n   *    0.25 YB - Validation successful: 402 rows in Yellowbrick table store\n   *    2.03    - Finished processing table ybaws.tpcds.store -> ms_test.tpcds.store\n\nProcessing table ybaws.tpcds.store_returns -> ms_test.tpcds.store_returns\n        Secs DW - Event\n   =====================================================\n   *    0.10    - Read table DDL from file ./ddl/store_returns.ddl\n   *    0.02 YB - Recreating store_returns\n   *    0.08 YB - -- Dropped store_returns\n   *    0.08 YB - -- Created Table store_returns\n             DB - Loading up to 2000000 rows of Spark table\n   *    0.43 DB - Received 2000000 rows from table: ybaws.tpcds.store_returns\n   *    4.53 DB - Exported 2000000 rows to CSV directory: s3a://ms-dbtest/store_returns.csv.\n             YB - Loading S3 files to store_returns\n   *    6.38 YB - Copied all parts to store_returns. 313,244 RPS\n   *    0.34 YB - Validation successful: 2000000 rows in Yellowbrick table store_returns\n   *   12.52    - Finished processing table ybaws.tpcds.store_returns -> ms_test.tpcds.store_returns\n\nProcessing table ybaws.tpcds.store_sales -> ms_test.tpcds.store_sales\n        Secs DW - Event\n   =====================================================\n   *    0.13    - Read table DDL from file ./ddl/store_sales.ddl\n   *    0.02 YB - Recreating store_sales\n   *    0.09 YB - -- Dropped store_sales\n   *    0.09 YB - -- Created Table store_sales\n             DB - Loading up to 2000000 rows of Spark table\n   *    0.57 DB - Received 2000000 rows from table: ybaws.tpcds.store_sales\n   *    4.71 DB - Exported 2000000 rows to CSV directory: s3a://ms-dbtest/store_sales.csv.\n             YB - Loading S3 files to store_sales\n   *    6.80 YB - Copied all parts to store_sales. 293,973 RPS\n   *    0.30 YB - Validation successful: 2000000 rows in Yellowbrick table store_sales\n   *   13.48    - Finished processing table ybaws.tpcds.store_sales -> ms_test.tpcds.store_sales\n\nAll tables processed successfully.\n"
     ]
    }
   ],
   "source": [
    "DB2YB(table_patterns = ['store.*','catalog.*'],  read_ddl = './ddl', limit=2000000)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DBCopyV2 Public",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}